% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fit_bernoulli_NN.R
\name{fit_bernoulli_nn}
\alias{fit_bernoulli_nn}
\title{Build and train a partially-interpretable neural network to fit a logistic regression model}
\usage{
fit_bernoulli_nn(
  Y_train,
  Y_test = NULL,
  X_train_nn,
  X_train_lin,
  X_train_add_basis,
  type = "MLP",
  n.ep = 100,
  batch.size = 100,
  init.p = 0.5,
  widths = c(6, 3),
  filter.dim = c(3, 3),
  seed = NULL
)
}
\arguments{
\item{Y_train, Y_test}{A 2 or 3 dimensional array of training or test response values, with entries of 0/1 for failure/success.
Missing values can be handled by setting corresponding entries to \code{Y_train} or \code{Y_test} to \code{-1e5}.
The first dimension should be the observation indices, e.g., time.
If \code{type=="CNN"}, then \code{Y_train} and \code{Y_test} must have three dimensions with the latter two corresponding to an \eqn{M} by \eqn{N} regular grid of spatial locations.
If \code{Y_test==NULL}, no validation loss will be computed and the returned model will be that which minimises the training loss over \code{n.ep} epochs.}

\item{X_train_nn}{A 3 or 4 dimensional array of "non-additive" predictor values.  If \code{NULL}, a model without the NN component is built and trained; if this is the case, then \code{type} has no efect.
The first 2/3 dimensions should be equal to that of \code{Y_train}; the last dimension corresponds to the chosen \eqn{d-l-a} 'non-additive' predictor values.}

\item{X_train_lin}{A 3 or 4 dimensional array of "linear" predictor values. Same number of dimensions as \code{X_train_nn}. If \code{NULL}, a model without the linear component is built and trained.
The first 2/3 dimensions should be equal to that of \code{Y_train}; the last dimension corresponds to the chosen \eqn{l} 'linear' predictor values.}

\item{X_train_add_basis}{A 4 or 5 dimensional array of basis function evaluations for the "additive" predictor values.
The first 2/3 dimensions should be equal to that of \code{Y_train}; the penultimate dimensions corresponds to the chosen \eqn{a} 'linear' predictor values and the last dimension is equal to the number of knots used for estimating the splines. See example.
If \code{NULL}, a model without the additive component is built and trained.}

\item{type}{A string defining the type of network to be built. If \code{type=="MLP"}, the network will have all densely connected layers; if \code{type=="CNN"}, the network will have all convolutional layers. Defaults to an MLP.}

\item{n.ep}{Number of epochs used for training. Defaults to 1000.}

\item{batch.size}{Batch size for stochastic gradient descent. If larger than \code{dim(Y_train)[1]}, i.e., the number of observations, then regular gradient descent used.}

\item{init.p}{Sets the initial probability estimate across all dimensions of \code{Y_train}.}

\item{widths}{Vector of widths/filters for hidden dense/convolution layers. Number of layers is equal to \code{length(widths)}. Defaults to 0.5.}

\item{filter.dim}{If \code{type=="CNN"}, this 2-vector gives the dimensions of the convolution filter kernel. The same filter is applied for each hidden layer.}

\item{seed}{Seed for random initial weights and biases.}
}
\value{
Returns the fitted model
}
\description{
Build and train a partially-interpretable neural network to fit a logistic regression model
}
\details{
Model is fitted by minimising the binary cross-entropy loss over \code{n.ep} epochs, with a logistic link function used to ensure that the estimated probabilities are in \eqn{(0,1)}.
Although the model is trained by minimising the loss evaluated for \code{Y_train}, the final returned model may minimise some other loss.
The current state of the model is saved after each epoch, using \code{keras::callback_model_checkpoint}, if the value of some criterion subcedes that of the model from the previous checkpoint; this criterion is the loss evaluated for validation/test set \code{Y_test} if \code{!is.null(Y_test)} and for \code{Y_train}, otherwise.
}
\examples{

##Build and train a simple MLP for toy data
X_train_nn<-rnorm(2000); X_train_add<-rnorm(2000); X_test_lin<-rnorm(2000) # Create 'nn', 'additive' and 'linear' predictors

dim(X_train_nn)=c(10,10,2,10) #Re-shape to a 4-d array. First dimension corresponds to observations, last to the different components of the predictor set
dim(X_train_lin)=c(10,10,2,10)
dim(X_train_add)=c(10,10,2,10)

#To build a model with an additive component, we require an array of evaluations of the basis functions for each pre-specified knot and entry to X_train_add

rad=function(x,c){ #Define a basis function. Here we use the radial bases
out=abs(x-c)^2*log(abs(x-c))
out[(x-c)==0]=0
return(out)
}

n.knot = 5 # set number of knots. Must be the same for each additive predictor
knots=matrix(nrow=dim(X_train_add)[4],ncol=n.knot)

#We set knots to be equally-spaced marginal quantiles
for( i in 1:dim(X_train_add)[4]) knots[i,]=quantile(X_train_add[,,,i],probs=seq(0,1,length=n.knot))

X_train_add_basis<-array(dim=c(dim(X_train_add),n.knot))
for( i in 1:dim(X_train_add)[4]) for(k in 1:n.knot)  X_train_add_basis[,,,i,k]= rad(x=X_train_add[,,,i],c=knots[i,k]) #Evaluate rad at all entries to X_train_add and for all knots

Y_train<-Y_test<-rnorm(200) #Create response data; training and test, respectively
dim(Y_train)=c(10,10,2) #Re-shape to a 4-d array. First dimension corresponds to observations
dim(Y_test)=c(10,10,2)

#Build and train a two-layered "lin+GAM+NN" MLP
model<-fit_bernoulli_nn(Y_train, Y_test ,X_train_nn,X_train_lin,X_train_add_basis, type="MLP",n.ep=100, batch.size=50,init.p=0.5, widths=c(6,3))



}
