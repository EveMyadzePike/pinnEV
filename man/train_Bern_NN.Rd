% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/train_Bern_NN.R
\name{train_Bern_NN}
\alias{train_Bern_NN}
\alias{predict_Bern_nn}
\title{Build and train a partially-interpretable neural network for a logistic regression model}
\usage{
train_Bern_NN(
  Y_train,
  Y_test = NULL,
  X_train,
  type = "MLP",
  n.ep = 100,
  batch.size = 100,
  init.p = 0.5,
  widths = c(6, 3),
  filter.dim = c(3, 3),
  seed = NULL
)

predict_Bern_nn(X_train, model)
}
\arguments{
\item{Y_train, Y_test}{A 2 or 3 dimensional array of training or test response values, with entries of 0/1 for failure/success.
Missing values can be handled by setting corresponding entries to \code{Y_train} or \code{Y_test} to \code{-1e5}.
The first dimension should be the observation indices, e.g., time.

If \code{type=="CNN"}, then \code{Y_train} and \code{Y_test} must have three dimensions with the latter two corresponding to an \eqn{M} by \eqn{N} regular grid of spatial locations.
If \code{Y_test==NULL}, no validation loss will be computed and the returned model will be that which minimises the training loss over \code{n.ep} epochs.}

\item{X_train}{A list of arrays corresponding to complementary subsets of the \eqn{d\geq 1} predictors which are used for modelling. Must contain at least one of the following three named entries:\describe{
\item{\code{X_train_lin}}{A 3 or 4 dimensional array of "linear" predictor values. Same number of dimensions as \code{X_train_nn}. If \code{NULL}, a model without the linear component is built and trained.
The first 2/3 dimensions should be equal to that of \code{Y_train}; the last dimension corresponds to the chosen \eqn{l\geq 0} 'linear' predictor values.}
\item{\code{X_train_add_basis}}{A 4 or 5 dimensional array of basis function evaluations for the "additive" predictor values.
The first 2/3 dimensions should be equal to that of \code{Y_train}; the penultimate dimensions corresponds to the chosen \eqn{a\geq 0} 'linear' predictor values and the last dimension is equal to the number of knots used for estimating the splines. See example.
If \code{NULL}, a model without the additive component is built and trained.}
\item{\code{X_train_nn}}{A 3 or 4 dimensional array of "non-additive" predictor values.  If \code{NULL}, a model without the NN component is built and trained; if this is the case, then \code{type} has no efect.
The first 2/3 dimensions should be equal to that of \code{Y_train}; the last dimension corresponds to the chosen \eqn{d-l-a\geq 0} 'non-additive' predictor values.}
}
Note that \code{X_train} is the predictors for both \code{Y_train} and \code{Y_test}.}

\item{type}{A string defining the type of network to be built. If \code{type=="MLP"}, the network will have all densely connected layers; if \code{type=="CNN"}, the network will have all convolutional layers. Defaults to an MLP.}

\item{n.ep}{Number of epochs used for training. Defaults to 1000.}

\item{batch.size}{Batch size for stochastic gradient descent. If larger than \code{dim(Y_train)[1]}, i.e., the number of observations, then regular gradient descent used.}

\item{init.p}{Sets the initial probability estimate across all dimensions of \code{Y_train}.}

\item{widths}{Vector of widths/filters for hidden dense/convolution layers. Number of layers is equal to \code{length(widths)}. Defaults to 0.5.}

\item{filter.dim}{If \code{type=="CNN"}, this 2-vector gives the dimensions of the convolution filter kernel. The same filter is applied for each hidden layer.}

\item{seed}{Seed for random initial weights and biases.}

\item{model}{Fitted \code{keras} model. Output from \code{train_Bern_NN}.}
}
\value{
\code{train_Bern_NN} returns the fitted \code{model}.  \code{predict_Bern_nn} is a wrapper for \code{keras::predict} that returns the predicted probability estimates, and, if applicable, the linear regression coefficients and spline bases weights.
}
\description{
Build and train a partially-interpretable neural network for a logistic regression model
}
\details{
{
Consider a Bernoulli random variable, say \eqn{Z\sim\mbox{Bernoulli}(p)}, with probability mass function \eqn{\Pr(Z=1)=p=1-\Pr(Z=0)=1-p}. Let \eqn{Y\in\{0,1\}} be a univariate Boolean response and let \eqn{\mathbf{X}} denote a \eqn{d}-dimensional predictor set with observations \eqn{\mathbf{x}}.
For integers \eqn{l\geq 0,a \geq 0} and \eqn{0\leq l+a \leq d}, let \eqn{\mathbf{X}_L, \mathbf{X}_A} and \eqn{\mathbf{X}_N} be distinct sub-vectors of \eqn{\mathbf{X}}, with observations of each component denoted \eqn{\mathbf{x}_L, \mathbf{x}_A} and \eqn{\mathbf{x}_N}, respectively; the lengths of the sub-vectors are \eqn{l,a} and \eqn{d-l-a}, respectively.
We model \eqn{Y|\mathbf{X}=\mathbf{X}\sim\mbox{Bernoulli}(p(\mathbf{x}))} with
\deqn{p(\mathbf{x})=h[\eta_0+m_L\{\mathbf{x}_L\}+m_A\{x_A\}+m_N\{\mathbf{x}_N\}],} where \eqn{h} is the logistic link-function and \eqn{\eta_0} is a constant intercept. The unknown functions \eqn{m_L} and \eqn{m_A} are estimated using a linear function and spline, respectively, and are both returned as outputs; \eqn{m_N} is estimated using a neural network.

The model is fitted by minimising the binary cross-entropy loss over \code{n.ep} training epochs.
Although the model is trained by minimising the loss evaluated for \code{Y_train}, the final returned model may minimise some other loss.
The current state of the model is saved after each epoch, using \code{keras::callback_model_checkpoint}, if the value of some criterion subcedes that of the model from the previous checkpoint; this criterion is the loss evaluated for validation/test set \code{Y_test} if \code{!is.null(Y_test)} and for \code{Y_train}, otherwise.

}
}
\examples{



# Build and train a simple MLP for toy data

# Create 'nn', 'additive' and 'linear' predictors
X_train_nn<-rnorm(5000); X_train_add<-rnorm(2000); X_train_lin<-rnorm(3000)

#Re-shape to a 4d array. First dimension corresponds to observations,
#last to the different components of the predictor set
dim(X_train_nn)=c(10,10,10,5)
dim(X_train_lin)=c(10,10,10,3)
dim(X_train_add)=c(10,10,10,2)

# Create toy response data

#Linear contribution
m_L = 0.3*X_train_lin[,,,1]+0.6*X_train_lin[,,,2]-0.2*X_train_lin[,,,3]

# Additive contribution
m_A = 0.1*X_train_add[,,,1]^2+0.2*X_train_add[,,,1]-0.1*X_train_add[,,,2]^3+0.5*X_train_add[,,,2]^2

#Non-additive contribution - to be estimated by NN
m_N = exp(-3+X_train_nn[,,,2]+X_train_nn[,,,3])
+sin(X_train_nn[,,,1]-X_train_nn[,,,2])*(X_train_nn[,,,1]+X_train_nn[,,,2])

p=0.5+0.5*tanh((m_L+m_A+m_N)/2) #Logistic link
Y=apply(p,1:3,function(x) rbinom(1,1,x))

#Create training and test, respectively.
#We mask 20\% of the Y values and use this for validation/testing.
#Masked values must be set to -1e5 and are treated as missing whilst training

mask_inds=sample(1:length(Y),size=length(Y)*0.8)

Y_train<-Y_test<-Y #Create training and test, respectively.
Y_train[-mask_inds]=-1e5
Y_test[mask_inds]=-1e5



#To build a model with an additive component, we require an array of evaluations of
#the basis functions for each pre-specified knot and entry to X_train_add

rad=function(x,c){ #Define a basis function. Here we use the radial bases
  out=abs(x-c)^2*log(abs(x-c))
  out[(x-c)==0]=0
  return(out)
}

n.knot = 5 # set number of knots. Must be the same for each additive predictor
knots=matrix(nrow=dim(X_train_add)[4],ncol=n.knot)

#We set knots to be equally-spaced marginal quantiles
for( i in 1:dim(X_train_add)[4]) knots[i,]=quantile(X_train_add[,,,i],probs=seq(0,1,length=n.knot))

X_train_add_basis<-array(dim=c(dim(X_train_add),n.knot))
for( i in 1:dim(X_train_add)[4]) {
for(k in 1:n.knot) {
X_train_add_basis[,,,i,k]= rad(x=X_train_add[,,,i],c=knots[i,k])
#Evaluate rad at all entries to X_train_add and for all knots
}}

X_train=list("X_train_nn"=X_train_nn, "X_train_lin"=X_train_lin,"X_train_add_basis"=NULL)

#Build and train a two-layered "lin+GAM+NN" MLP
model<-train_Bern_NN(Y_train, Y_test,X_train,  type="MLP",n.ep=2000,
                      batch.size=50,init.p=0.4, widths=c(6,3))

out<-predict_bernoulli_nn(X_train,model)
print(out$lin.coeff)


}
