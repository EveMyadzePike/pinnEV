% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/train_bGEVPP_NN.R
\name{train_bGEVPP_NN}
\alias{train_bGEVPP_NN}
\title{Build and train a partially-interpretable neural network for fitting a bGEV point-process model}
\usage{
train_bGEVPP_NN(
  Y_train,
  Y_test = NULL,
  X_train,
  type = "MLP",
  link = "identity",
  tau = NULL,
  n.ep = 100,
  batch.size = 100,
  init.q = NULL,
  widths = c(6, 3),
  filter.dim = c(3, 3),
  seed = NULL
)
}
\arguments{
\item{Y_train, Y_test}{A 2 or 3 dimensional array of training or test real response values.
Missing values can be handled by setting corresponding entries to \code{Y_train} or \code{Y_test} to \code{-1e5}.
The first dimension should be the observation indices, e.g., time.

If \code{type=="CNN"}, then \code{Y_train} and \code{Y_test} must have three dimensions with the latter two corresponding to an \eqn{M} by \eqn{N} regular grid of spatial locations.
If \code{Y_test==NULL}, no validation loss will be computed and the returned model will be that which minimises the training loss over \code{n.ep} epochs.}

\item{X_train}{A list of arrays corresponding to complementary subsets of the \eqn{d\geq 1} predictors which are used for modelling. Must contain at least one of the following three named entries:\describe{
\item{\code{X_train_lin}}{A 3 or 4 dimensional array of "linear" predictor values. Same number of dimensions as \code{X_train_nn}. If \code{NULL}, a model without the linear component is built and trained.
The first 2/3 dimensions should be equal to that of \code{Y_train}; the last dimension corresponds to the chosen \eqn{l\geq 0} 'linear' predictor values.}
\item{\code{X_train_add_basis}}{A 4 or 5 dimensional array of basis function evaluations for the "additive" predictor values.
The first 2/3 dimensions should be equal to that of \code{Y_train}; the penultimate dimensions corresponds to the chosen \eqn{a\geq 0} 'linear' predictor values and the last dimension is equal to the number of knots used for estimating the splines. See example.
If \code{NULL}, a model without the additive component is built and trained.}
\item{\code{X_train_nn}}{A 3 or 4 dimensional array of "non-additive" predictor values.  If \code{NULL}, a model without the NN component is built and trained; if this is the case, then \code{type} has no efect.
The first 2/3 dimensions should be equal to that of \code{Y_train}; the last dimension corresponds to the chosen \eqn{d-l-a\geq 0} 'non-additive' predictor values.}
}
Note that \code{X_train} is the predictors for both \code{Y_train} and \code{Y_test}.}

\item{type}{A string defining the type of network to be built. If \code{type=="MLP"}, the network will have all densely connected layers; if \code{type=="CNN"}, the network will have all convolutional layers. Defaults to an MLP.}

\item{tau}{Considered quantile level. Must satisfy \code{0 < tau < 1}.}

\item{n.ep}{Number of epochs used for training. Defaults to 1000.}

\item{batch.size}{Batch size for stochastic gradient descent. If larger than \code{dim(Y_train)[1]}, i.e., the number of observations, then regular gradient descent used.}

\item{init.q}{Sets the initial \code{tau}-quantile estimate across all dimensions of \code{Y_train}. Defaults to empirical estimate.}

\item{widths}{Vector of widths/filters for hidden dense/convolution layers. Number of layers is equal to \code{length(widths)}. Defaults to (6,3).}

\item{filter.dim}{If \code{type=="CNN"}, this 2-vector gives the dimensions of the convolution filter kernel. The same filter is applied for each hidden layer.}

\item{seed}{Seed for random initial weights and biases.}

\item{u_train}{An array with the same dimension of \code{Y_train}. Gives the quantile above which the bGEV-PP model is fitted, see below. Note that \code{u_train} is applies to both \code{Y_train} and \code{Y_test}.}

\item{loc.link}{A string defining the link function used for the location parameter, see \eqn{h_1} below. If \code{link=="exp"}, then \eqn{h_1=\exp(x)}; if \code{link=="identity"}, then \eqn{h_1(x)=x}.}

\item{model}{Fitted \code{keras} model. Output from \code{train_bGEVPP_NN}.}
}
\value{
\code{train_bGEVPP_NN} returns the fitted \code{model}.  \code{predict_bGEVPP_nn} is a wrapper for \code{keras::predict} that returns the predicted parameter estimates, and, if applicable, their corresponding linear regression coefficients and spline bases weights.
}
\description{
Build and train a partially-interpretable neural network for fitting a bGEV point-process model
}
\details{
{
Consider a real-valued random variable \eqn{Y} and let \eqn{\mathbf{X}} denote a \eqn{d}-dimensional predictor set with observations \eqn{\mathbf{x}}.
For \eqn{i=1,2}, we define integers \eqn{l_i\geq 0,a_i \geq 0} and \eqn{0\leq l_i+a_i \leq d}, and let \eqn{\mathbf{X}^{(i)}_L, \mathbf{X}^{(i)}_A} and \eqn{\mathbf{X}^{(i)}_N} be distinct sub-vectors
of \eqn{\mathbf{X}}, with observations of each component denoted \eqn{\mathbf{x}^{(i)}_L, \mathbf{x}^{(i)}_A} and \eqn{\mathbf{x}^{(i)}_N}, respectively; the lengths of the sub-vectors are \eqn{l_i,a_i} and \eqn{d_i-l_i-a}, respectively.
For a fixed threshold \eqn{u(\mathbf{x})}, dependent on predictors, we model \eqn{Y|\mathbf{X}=\mathbf{x}\sim\mbox{bGEV-PP}(q_\alpha(\mathbf{x}),s_\beta(\mathbf{x}),\xi>0;u(\mathbf{x}))} with
\deqn{q_\alpha (\mathbf{x})=h_1[\eta^{(1)}_0+m^{(1)}_L\{\mathbf{x}^{(1)}_L\}+m^{(1)}_A\{x^{(1)}_A\}+m^{(1)}_N\{\mathbf{x}^{(1)}_N\}]} and
\deqn{s_\beta (\mathbf{x})=\exp[\eta^{(2)}_0+m^{(2)}_L\{\mathbf{x}^{(2)}_L\}+m^{(2)}_A\{x^{(2)}_A\}+m^{(2)}_N\{\mathbf{x}^{(2)}_N\}]}
where \eqn{h_1} is some link-function and \eqn{\eta^{(1)}_0,\eta^{(2)}_0} are constant intercepts. The unknown functions \eqn{m^{(1)}_L,m^{(2)}_L} and \eqn{m^{(1)}_A,m^{(2)}_A} are estimated using a linear function and spline, respectively, and are
both returned as outputs; \eqn{m^{(1)}_N,m^{(2)}_N} are estimated using neural networks (currently the same architecture is used for both).

The model is fitted by minimising the negative log-likelihood associated with the bGEV-PP model over \code{n.ep} training epochs.
Although the model is trained by minimising the loss evaluated for \code{Y_train}, the final returned model may minimise some other loss.
The current state of the model is saved after each epoch, using \code{keras::callback_model_checkpoint}, if the value of some criterion subcedes that of the model from the previous checkpoint; this criterion is the loss evaluated for validation/test set \code{Y_test} if \code{!is.null(Y_test)} and for \code{Y_train}, otherwise.

}
}
\examples{


# Build and train a simple MLP for toy data

# Create 'nn', 'additive' and 'linear' predictors
X_train_nn<-rnorm(5000); X_train_add<-rnorm(2000); X_train_lin<-rnorm(3000)

#Re-shape to a 4d array. First dimension corresponds to observations,
#last to the different components of the predictor set
dim(X_train_nn)=c(10,10,10,5) #Five nn predictors
dim(X_train_lin)=c(10,10,10,3) #Three linear predictors
dim(X_train_add)=c(10,10,10,2) #Two additive predictors

# Create toy response data

#Linear contribution
m_L = 0.3*X_train_lin[,,,1]+0.6*X_train_lin[,,,2]-0.2*X_train_lin[,,,3]

# Additive contribution
m_A = 0.1*X_train_add[,,,1]^2+0.2*X_train_add[,,,1]-0.1*X_train_add[,,,2]^3+0.5*X_train_add[,,,2]^2

#Non-additive contribution - to be estimated by NN
m_N = exp(-3+X_train_nn[,,,2]+X_train_nn[,,,3])
+sin(X_train_nn[,,,1]-X_train_nn[,,,2])*(X_train_nn[,,,1]+X_train_nn[,,,2])

theta=1+m_L+m_A+m_N #Identity link
#We simulate normal data and estimate the median, i.e., the 50\% quantile or mean,
#as the form for this is known
Y=apply(theta,1:3,function(x) rnorm(1,mean=x,sd=2))

#Create training and test, respectively.
#We mask 20\% of the Y values and use this for validation/testing.
#Masked values must be set to -1e5 and are treated as missing whilst training

mask_inds=sample(1:length(Y),size=length(Y)*0.8)

Y_train<-Y_test<-Y #Create training and test, respectively.
Y_train[-mask_inds]=-1e5
Y_test[mask_inds]=-1e5



#To build a model with an additive component, we require an array of evaluations of
#the basis functions for each pre-specified knot and entry to X_train_add

rad=function(x,c){ #Define a basis function. Here we use the radial bases
  out=abs(x-c)^2*log(abs(x-c))
  out[(x-c)==0]=0
  return(out)
}

n.knot = 5 # set number of knots. Must be the same for each additive predictor
knots=matrix(nrow=dim(X_train_add)[4],ncol=n.knot)

#We set knots to be equally-spaced marginal quantiles
for( i in 1:dim(X_train_add)[4]) knots[i,]=quantile(X_train_add[,,,i],probs=seq(0,1,length=n.knot))

X_train_add_basis<-array(dim=c(dim(X_train_add),n.knot))
for( i in 1:dim(X_train_add)[4]) {
for(k in 1:n.knot) {
X_train_add_basis[,,,i,k]= rad(x=X_train_add[,,,i],c=knots[i,k])
#Evaluate rad at all entries to X_train_add and for all knots
}}

X_train=list("X_train_nn"=X_train_nn, "X_train_lin"=X_train_lin,"X_train_add_basis"=X_train_add_basis)

#Build and train a two-layered "lin+GAM+NN" MLP
tau <- 0.5 # tau must be set as a global variable to pass it to the keras custom loss function
model<-train_quant_NN(Y_train, Y_test,X_train,  type="MLP",link="identity",tau=0.5,n.ep=50,
                      batch.size=50, widths=c(6,3))

out<-predict_quant_nn(X_train,model)
hist(out$predictions) #Plot histogram of predicted quantiles
print(out$lin.coeff)

n.add.preds=dim(X_train_add)[length(dim(X_train_add))]
par(mfrow=c(1,n.add.preds))
for(i in 1:n.add.preds){
 plt.x=seq(from=min(knots[i,]),to=max(knots[i,]),length=1000)  #Create sequence for x-axis

 tmp=matrix(nrow=length(plt.x),ncol=n.knot)
 for(j in 1:n.knot){
   tmp[,j]=rad(plt.x,knots[i,j]) #Evaluate radial basis function of plt.x and all knots
 }
 plt.y=tmp\%*\%out$gam.weights[i,]
 plot(plt.x,plt.y,type="l",main=paste0("Quantile spline: predictor ",i),xlab="x",ylab="f(x)")
 points(knots[i,],rep(mean(plt.y),n.knot),col="red",pch=2) #Adds red triangles that denote knot locations
}


#To save model, run model \%>\% save_model_tf(paste0("model_",tau,"-quantile"))
#To load model, run model  <- load_model_tf(paste0("model_",tau,"-quantile"),
#custom_objects=list("tilted_loss"=tilted_loss))

}
